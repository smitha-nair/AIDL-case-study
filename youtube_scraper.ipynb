{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smitha-nair/AIDL-case-study/blob/main/youtube_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ablX9AHyaVOt",
        "outputId": "81640a2f-c212-4451-adc7-aa44e24b48c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-comment-downloader\n",
            "  Downloading youtube_comment_downloader-0.1.76-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dateparser (from youtube-comment-downloader)\n",
            "  Downloading dateparser-1.2.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-comment-downloader) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from dateparser->youtube-comment-downloader) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser->youtube-comment-downloader) (2024.2)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser->youtube-comment-downloader) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser->youtube-comment-downloader) (5.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-comment-downloader) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-comment-downloader) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-comment-downloader) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-comment-downloader) (2024.12.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->dateparser->youtube-comment-downloader) (1.17.0)\n",
            "Downloading youtube_comment_downloader-0.1.76-py3-none-any.whl (8.2 kB)\n",
            "Downloading dateparser-1.2.0-py2.py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dateparser, youtube-comment-downloader\n",
            "Successfully installed dateparser-1.2.0 youtube-comment-downloader-0.1.76\n"
          ]
        }
      ],
      "source": [
        "#to download comments from YouTube videos\n",
        "!pip install youtube-comment-downloader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "5SeiWHbGbWV5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytb_video_list = ['https://youtu.be/0sOvCWFmrtA']\n",
        " #             'https://www.youtube.com/watch?v=TuIgtitqJho']\n",
        "#                  'https://www.youtube.com/watch?v=hinZO--TEk4']"
      ],
      "metadata": {
        "id": "qG9lYXIubWYH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vid_id_separator(ytb_video_list):\n",
        "  list1=[]\n",
        "  for item in ytb_video_list:\n",
        "    if re.search('youtu.be',item):\n",
        "      item_list=item.split('/')\n",
        "      print(\"item_list\",item_list)\n",
        "      list1.append(item_list[3])\n",
        "    if re.search('www.youtube.com',item):\n",
        "      item_list=item.split('v=')\n",
        "      list1.append(item_list[1])\n",
        "  #print(\"list1....\",list1)\n",
        "  return list1\n",
        "def cmt_dow(list1):\n",
        "  for item in list1:\n",
        "    print(item)\n",
        "    !youtube-comment-downloader --youtubeid {item} --output {item}.json\n",
        "def csv_ext(id_list):\n",
        "  #print(\"id_list....\",id_list)\n",
        "  df=pd.DataFrame()\n",
        " # df = pd.DataFrame(columns=list(\"Text\"))\n",
        "\n",
        "  #print(\"df ........\",df)\n",
        "  fname=[]\n",
        "  for item in id_list:\n",
        "    ll='/content/'+item+'.json'\n",
        "    fname.append(ll)\n",
        "  #print(\"fname...\",fname)\n",
        "  for fname in fname:\n",
        "    with open(fname, \"r\") as a_file:\n",
        "      for line in a_file:\n",
        "   #     print(\"line...\",line)\n",
        "        res = json.loads(line) #to deserialize a JSON string (text) into a Python dictionary or list.\n",
        "        list_s=[res['text']]\n",
        "  #      print(\"list_s....\",list_s)\n",
        "  #      print(\"type of list_s\", type(list_s))\n",
        "  #      df= df.append(pd.DataFrame([list_s], columns=[\"Text\"]), ignore_index=True)\n",
        "        df =  pd.concat([df, pd.DataFrame(list_s)], ignore_index=True)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "TMTM4jEjPNVy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_function(ytb_video_list):\n",
        "  inp1=vid_id_separator(ytb_video_list)\n",
        "  print(\"inp1...\",inp1)\n",
        "  cmt_dow(inp1)\n",
        "  inp3=csv_ext(inp1)\n",
        "  print(\"inp3....\",inp3)\n",
        "  return inp3"
      ],
      "metadata": {
        "id": "tZ7xrYZucICz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=final_function(ytb_video_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "802qpLVMluAZ",
        "outputId": "8624dc26-7419-416a-f0d9-ff88d7a5ab79"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "item_list ['https:', '', 'youtu.be', '0sOvCWFmrtA']\n",
            "inp1... ['0sOvCWFmrtA']\n",
            "0sOvCWFmrtA\n",
            "Downloading Youtube comments for 0sOvCWFmrtA\n",
            "Downloaded 2318 comment(s)\n",
            "[98.86 seconds] Done!\n",
            "inp3....                                                       0\n",
            "0     This is the 1st vedio I saw from your channel ...\n",
            "1     Can't deploy at heroku. Maybe Credit card issu...\n",
            "2     from app import models, schemas, utils this is...\n",
            "3     4:31:10 No Diddy \\n\\n\\n\\n\\nJokes aside, great ...\n",
            "4     2:53:10 I'd recommend DevDocs, as it makes doc...\n",
            "...                                                 ...\n",
            "2313  Do police in Nigeria takes bribe for no real r...\n",
            "2314                                   Finally First!!!\n",
            "2315                                              First\n",
            "2316                                                ðŸ‘ŒðŸ»ðŸ”¥\n",
            "2317                                                 â¤ï¸\n",
            "\n",
            "[2318 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1)\n",
        "# Define the file path for the Excel file\n",
        "excel_file_path = 'data.xlsx'\n",
        "\n",
        "# Convert the DataFrame to an Excel file\n",
        "df1.to_excel(excel_file_path, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abwEDzmgB6w6",
        "outputId": "5c02bf1b-5b23-4517-bfc6-c302e2c09bfa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                      0\n",
            "0     This is the 1st vedio I saw from your channel ...\n",
            "1     Can't deploy at heroku. Maybe Credit card issu...\n",
            "2     from app import models, schemas, utils this is...\n",
            "3     4:31:10 No Diddy \\n\\n\\n\\n\\nJokes aside, great ...\n",
            "4     2:53:10 I'd recommend DevDocs, as it makes doc...\n",
            "...                                                 ...\n",
            "2313  Do police in Nigeria takes bribe for no real r...\n",
            "2314                                   Finally First!!!\n",
            "2315                                              First\n",
            "2316                                                ðŸ‘ŒðŸ»ðŸ”¥\n",
            "2317                                                 â¤ï¸\n",
            "\n",
            "[2318 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load the Excel file\n",
        "df = pd.read_excel('data.xlsx')\n",
        "\n",
        "# Initialize the sentiment analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Perform sentiment analysis and classification for each text entry\n",
        "sentiments = []\n",
        "for index, row in df.iterrows():\n",
        "    # Accessing the first column by index instead of by name 'text'\n",
        "    text = str(row[0])\n",
        "    sentiment_score = sid.polarity_scores(text)\n",
        "\n",
        "    if sentiment_score['compound'] >= 0.05:\n",
        "        sentiment_class = \"Positive\"\n",
        "    elif sentiment_score['compound'] <= -0.05:\n",
        "        sentiment_class = \"Negative\"\n",
        "    else:\n",
        "        sentiment_class = \"Neutral\"\n",
        "\n",
        "    # Append the sentiment analysis results to the list\n",
        "    sentiments.append({'Text': text, 'Sentiment': sentiment_score, 'Sentiment_Class': sentiment_class})\n",
        "\n",
        "# Convert the list of sentiment analysis results to a DataFrame\n",
        "sentiments_df = pd.DataFrame(sentiments)\n",
        "\n",
        "# Save the results to a new Excel file or perform further analysis\n",
        "sentiments_df.to_excel('sentiment_analysis_results.xlsx', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMyREAB6sLsA",
        "outputId": "10d37fc2-952a-4755-cb2e-1481c58393f6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}